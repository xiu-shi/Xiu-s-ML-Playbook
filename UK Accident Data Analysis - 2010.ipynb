UK Accident Data Analysis - 2010

This notebook presents a comprehensive analysis of the 2010 UK accident information dataset, focusing on machine learning approaches for accident severity prediction.


1. Introduction

Road traffic accidents are a significant public health concern worldwide. Understanding the factors that contribute to accident severity can help in developing effective prevention strategies. This analysis aims to identify the most important features for predicting accident severity and evaluate different classification approaches for this prediction task.

The specific objectives of this analysis are:

    Identify the most important features for predicting accident severity
    Determine the most suitable classification approach for severity prediction
    Implement SVM for binary classification of accident severity
    Analyze the benefits of dimensionality reduction in this context

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 8)
plt.rcParams["font.size"] = 12

2. Dataset Examination


# Check column types and missing values
print("Data types:")
print(df.dtypes)

print("\nMissing values:")
print(df.isnull().sum())

2.1 Target Variable Analysis

The target variable for our analysis is Accident_Severity, which categorizes accidents as Fatal, Serious, or Slight.


# Examine target variable distribution
severity_counts = df["Accident_Severity"].value_counts()
print("Accident Severity Distribution:")
print(severity_counts)
print("\nPercentage:")
print(severity_counts / len(df) * 100)

# Visualize target variable distribution
plt.figure(figsize=(10, 6))
sns.countplot(x="Accident_Severity", data=df, palette="viridis")
plt.title("Distribution of Accident Severity")
plt.xlabel("Severity")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig("severity_distribution.png")
plt.show()




# Convert date and time to usable features
df["Date"] = pd.to_datetime(df["Date"], format="%d/%m/%Y", errors="coerce")
df["Month"] = df["Date"].dt.month
df["Time"] = pd.to_datetime(df["Time"], format="%H:%M", errors="coerce")
df["Hour"] = df["Time"].dt.hour

# Temporal patterns
plt.figure(figsize=(12, 6))
monthly_accidents = df.groupby(["Month", "Accident_Severity"]).size().unstack()
monthly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Month and Severity")
plt.xlabel("Month")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("accidents_by_month.png")
plt.show()

plt.figure(figsize=(14, 6))
hourly_accidents = df.groupby(["Hour", "Accident_Severity"]).size().unstack()
hourly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Hour and Severity")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('accidents_by_hour.png')
plt.show()


# Relationship between severity and road conditions
plt.figure(figsize=(12, 6))
road_cond = pd.crosstab(df["Road_Surface_Conditions"], df["Accident_Severity"], normalize="index") * 100
road_cond.plot(kind="bar")
plt.title("Accident Severity by Road Surface Conditions")
plt.xlabel("Road Surface Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("severity_by_road_conditions.png")
plt.show()

# Relationship between severity and light conditions
plt.figure(figsize=(12, 6))
light_cond = pd.crosstab(df["Light_Conditions"], df["Accident_Severity"], normalize="index") * 100
light_cond.plot(kind="bar")
plt.title("Accident Severity by Light Conditions")
plt.xlabel("Light Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("severity_by_light_conditions.png")
plt.show()



# Relationship between severity and speed limit
plt.figure(figsize=(10, 6))
speed_sev = pd.crosstab(df["Speed_limit"], df["Accident_Severity"], normalize="index") * 100
speed_sev.plot(kind="bar")
plt.title("Accident Severity by Speed Limit")
plt.xlabel("Speed Limit (mph)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig("severity_by_speed_limit.png")
plt.show()

# Relationship between severity and urban/rural area
plt.figure(figsize=(10, 6))
area_sev = pd.crosstab(df["Urban_or_Rural_Area"], df["Accident_Severity"], normalize="index") * 100
area_sev.plot(kind="bar")
plt.title("Accident Severity by Area Type")
plt.xlabel("Area Type (1: Urban, 2: Rural)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig("severity_by_area.png")
plt.show()

3. Data Preprocessing

# Define target variable
severity_mapping = {'Fatal': 2, 'Serious': 1, 'Slight': 0}
df["Severity_Encoded"] = df["Accident_Severity"].map(severity_mapping)

# Create binary target variable (Severe vs. Non-severe)
df["Severity_Binary"] = (df["Severity_Encoded"] > 0).astype(int)

# Select features based on EDA
selected_features = [
    "Day_of_Week", "Hour", "Month", 
    "Road_Type", "Road_Surface_Conditions", "Light_Conditions",
    "Weather_Conditions", "Urban_or_Rural_Area", 
    "Speed_limit", "Number_of_Vehicles", "Number_of_Casualties",
    "Junction_Detail", "1st_Road_Class"
]

# Create feature matrix X and target vectors y
X = df[selected_features].copy()
y = df["Severity_Encoded"]  # For multiclass
y_binary = df["Severity_Binary"]  # For binary classification

# Identify categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical features:", categorical_features)
print("Numerical features:", numerical_features)

# Create preprocessing pipeline
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ('cat', categorical_transformer, categorical_features),
    ('num', numerical_transformer, numerical_features)
])

# Create multiple training splits
split_sizes = [0.05, 0.10, 0.20]
data_splits = {}

for split_size in split_sizes:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=1-split_size, random_state=42, stratify=y
    )
    
    # Apply preprocessing
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)
    
    data_splits[split_size] = {
        'X_train': X_train_processed,
        'X_test': X_test_processed,
        'y_train': y_train,
        'y_test': y_test
    }
    
    print(f"Split {split_size*100}%: Training samples = {X_train.shape[0]}, Test samples = {X_test.shape[0]}")


# 4. Feature Selection

#We'll apply multiple feature selection methods to identify the most important features for predicting #accident severity.

# Use the 20% split for feature selection
X_train_20pct = data_splits[0.20]['X_train']
y_train_20pct = data_splits[0.20]['y_train']

# 1. Univariate feature selection
k_best = 10  # Select top 10 features
selector_f = SelectKBest(f_classif, k=k_best)
X_kbest = selector_f.fit_transform(X_train_20pct, y_train_20pct)

# Get selected feature indices
kbest_indices = selector_f.get_support(indices=True)
print(f"Univariate Selection (top {k_best} features): {kbest_indices}")

# 2. Recursive Feature Elimination
rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=k_best)
rfe.fit(X_train_20pct, y_train_20pct)
rfe_indices = np.where(rfe.support_)[0]
print(f"RFE Selection (top {k_best} features): {rfe_indices}")

# 3. Random Forest feature importance
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_20pct, y_train_20pct)
importances = rf.feature_importances_
rf_indices = np.argsort(importances)[::-1][:k_best]
print(f"Random Forest Importance (top {k_best} features): {rf_indices}")

# Find common features across methods
common_indices = set(kbest_indices) & set(rfe_indices) & set(rf_indices)
print(f"\nCommon features across all methods: {common_indices}")

# Use union of features from all methods
selected_indices = sorted(list(set(kbest_indices) | set(rfe_indices) | set(rf_indices)))
print(f"\nSelected features (union): {selected_indices}")
print(f"Number of selected features: {len(selected_indices)}")



# 5. Model Development and Evaluation

# We'll develop and evaluate multiple classification models using the selected features.

# Define models to evaluate
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

# Function to evaluate models
def evaluate_model(model, X_train, y_train, X_test, y_test, cv=5):
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # Perform cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    
    return {
        'accuracy': accuracy,
        'cv_mean': np.mean(cv_scores),
        'cv_std': np.std(cv_scores),
        'report': report,
        'predictions': y_pred
    }

# Results storage
results = {}

# Evaluate models on different splits
for split_size, data in data_splits.items():
    # Extract data
    X_train = data['X_train']
    X_test = data['X_test']
    y_train = data['y_train']
    y_test = data['y_test']
    
    # Filter to selected features
    X_train_selected = X_train[:, selected_indices]
    X_test_selected = X_test[:, selected_indices]
    
    print(f"\nEvaluating models on {split_size*100}% training split")
    print(f"Number of selected features: {len(selected_indices)}")
    print(f"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")
    
    # Evaluate each model
    split_results = {}
    for name, model in models.items():
        print(f"Training {name}...")
        split_results[name] = evaluate_model(model, X_train_selected, y_train, X_test_selected, y_test)
        print(f"{name} - Test Accuracy: {split_results[name]['accuracy']:.4f}, CV Accuracy: {split_results[name]['cv_mean']:.4f} ± {split_results[name]['cv_std']:.4f}")
    
    results[split_size] = split_results


# Create summary DataFrame
summary_data = []
for split_size, split_results in results.items():
    for model_name, model_results in split_results.items():
        summary_data.append({
            'Split Size': f'{split_size*100}%',
            'Model': model_name,
            'Test Accuracy': model_results['accuracy'],
            'CV Mean Accuracy': model_results['cv_mean'],
            'CV Std': model_results['cv_std'],
            'Precision (weighted)': model_results['report']['weighted avg']['precision'],
            'Recall (weighted)': model_results['report']['weighted avg']['recall'],
            'F1 Score (weighted)': model_results['report']['weighted avg']['f1-score']
        })

summary_df = pd.DataFrame(summary_data)
summary_df



# Model comparison across splits
plt.figure(figsize=(14, 8))
for model_name in models.keys():
    model_data = summary_df[summary_df['Model'] == model_name]
    plt.plot(model_data['Split Size'], model_data['Test Accuracy'], marker='o', label=model_name)

plt.xlabel('Training Split Size')
plt.ylabel('Test Accuracy')
plt.title('Model Performance Across Different Training Split Sizes')
plt.legend()
plt.grid(True)
plt.savefig('model_comparison_splits.png')
plt.show()


# Model comparison for 20% split
plt.figure(figsize=(12, 8))
split_20_data = summary_df[summary_df['Split Size'] == '20.0%']
metrics = ['Test Accuracy', 'Precision (weighted)', 'Recall (weighted)', 'F1 Score (weighted)']
x = np.arange(len(models))
width = 0.2

for i, metric in enumerate(metrics):
    plt.bar(x + i*width, split_20_data[metric], width, label=metric)

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model Performance Metrics (20% Training Split)')
plt.xticks(x + width*1.5, split_20_data['Model'])
plt.legend()
plt.grid(axis='y')
plt.savefig('model_metrics_comparison.png')
plt.show()


# Confusion matrix for best model on 20% split
best_model_name = split_20_data.loc[split_20_data['Test Accuracy'].idxmax(), 'Model']
best_model = models[best_model_name]
X_train_20 = data_splits[0.20]['X_train'][:, selected_indices]
X_test_20 = data_splits[0.20]['X_test'][:, selected_indices]
y_train_20 = data_splits[0.20]['y_train']
y_test_20 = data_splits[0.20]['y_test']

best_model.fit(X_train_20, y_train_20)
y_pred_20 = best_model.predict(X_test_20)

cm = confusion_matrix(y_test_20, y_pred_20)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Slight', 'Serious', 'Fatal'],
            yticklabels=['Slight', 'Serious', 'Fatal'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title(f'Confusion Matrix - {best_model_name} (20% Training Split)')
plt.savefig('best_model_confusion_matrix.png')
plt.show()


# 6. Hyperparameter Optimization

# We'll optimize the hyperparameters of the best performing models using grid search with cross-validation.


from sklearn.model_selection import GridSearchCV

# Use the 20% split for hyperparameter optimization
X_train_selected = data_splits[0.20]['X_train'][:, selected_indices]
X_test_selected = data_splits[0.20]['X_test'][:, selected_indices]
y_train = data_splits[0.20]['y_train']
y_test = data_splits[0.20]['y_test']

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 1. Random Forest Hyperparameter Tuning
print('\n1. Random Forest Hyperparameter Tuning')
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=rf_param_grid,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

rf_grid.fit(X_train_selected, y_train)
print(f'Best parameters: {rf_grid.best_params_}')
print(f'Best cross-validation score: {rf_grid.best_score_:.4f}')

# Evaluate best model
best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test_selected)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f'Test accuracy with best parameters: {rf_accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred_rf))


# 2. SVM Hyperparameter Tuning
print('\n2. SVM Hyperparameter Tuning')
svm_param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1],
    'kernel': ['rbf', 'linear', 'poly']
}

svm_grid = GridSearchCV(
    SVC(probability=True, random_state=42),
    param_grid=svm_param_grid,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

svm_grid.fit(X_train_selected, y_train)
print(f'Best parameters: {svm_grid.best_params_}')
print(f'Best cross-validation score: {svm_grid.best_score_:.4f}')

# Evaluate best model
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test_selected)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
print(f'Test accuracy with best parameters: {svm_accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred_svm))


# Compare optimized models
print('\nComparison of Optimized Models:')
print(f'Random Forest: {rf_accuracy:.4f}')
print(f'SVM: {svm_accuracy:.4f}')

# Create confusion matrices
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Slight', 'Serious', 'Fatal'],
            yticklabels=['Slight', 'Serious', 'Fatal'])
plt.title('Random Forest Confusion Matrix')

plt.subplot(1, 2, 2)
cm_svm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Slight', 'Serious', 'Fatal'],
            yticklabels=['Slight', 'Serious', 'Fatal'])
plt.title('SVM Confusion Matrix')

plt.tight_layout()
plt.savefig('optimized_models_confusion_matrices.png')
plt.show()


### 7. SVM for Binary Classification (Loyal vs Churn Customers)

# In the context of the assignment, we'll interpret:

   # Loyal customers as Non-severe accidents (Slight = 0)
   # Churn customers as Severe accidents (Serious/Fatal = 1)


# Create binary target variable
y_train_binary = (y_train > 0).astype(int)
y_test_binary = (y_test > 0).astype(int)

print('\nBinary Class Distribution:')
print('Training set:')
print(pd.Series(y_train_binary).value_counts())
print('\nTest set:')
print(pd.Series(y_test_binary).value_counts())

# Define SVM parameter grid for binary classification
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1],
    'kernel': ['rbf', 'linear', 'poly'],
    'class_weight': ['balanced', None]
}

# Perform grid search
print('\nPerforming Grid Search for SVM hyperparameters...')
svm_grid = GridSearchCV(
    SVC(probability=True, random_state=42),
    param_grid=param_grid,
    cv=cv,
    scoring='f1',  # Using F1 score to balance precision and recall
    n_jobs=-1,
    verbose=1
)

svm_grid.fit(X_train_selected, y_train_binary)

print(f'\nBest parameters: {svm_grid.best_params_}')
print(f'Best cross-validation score: {svm_grid.best_score_:.4f}')



# Evaluate best model
best_svm = svm_grid.best_estimator_
y_pred_binary = best_svm.predict(X_test_selected)
y_prob_binary = best_svm.predict_proba(X_test_selected)[:, 1]

# Calculate metrics
accuracy = accuracy_score(y_test_binary, y_pred_binary)
print(f'\nTest accuracy: {accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test_binary, y_pred_binary, target_names=['Non-severe', 'Severe']))


# Create confusion matrix
cm = confusion_matrix(y_test_binary, y_pred_binary)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Non-severe', 'Severe'],
            yticklabels=['Non-severe', 'Severe'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('SVM Binary Classification Confusion Matrix')
plt.savefig('svm_binary_confusion_matrix.png')
plt.show()

# Create ROC curve
from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(y_test_binary, y_prob_binary)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for SVM')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.7)
plt.savefig('svm_binary_roc_curve.png')
plt.show()


# Compare with Random Forest for binary classification
# Convert the multiclass RF predictions to binary
y_pred_rf_multiclass = best_rf.predict(X_test_selected)
y_pred_rf_binary = (y_pred_rf_multiclass > 0).astype(int)
y_prob_rf_binary = best_rf.predict_proba(X_test_selected)[:, 1] + best_rf.predict_proba(X_test_selected)[:, 2]

# Calculate metrics for RF
rf_accuracy = accuracy_score(y_test_binary, y_pred_rf_binary)
print(f'\nRandom Forest Binary Classification Accuracy: {rf_accuracy:.4f}')
print('\nRandom Forest Binary Classification Report:')
print(classification_report(y_test_binary, y_pred_rf_binary, target_names=['Non-severe', 'Severe']))


# Create ROC curve comparison
fpr_rf, tpr_rf, _ = roc_curve(y_test_binary, y_prob_rf_binary)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'SVM (AUC = {roc_auc:.2f})')
plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison: SVM vs Random Forest')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.7)
plt.savefig('binary_classification_roc_comparison.png')
plt.show()


### 8. Dimensionality Reduction Analysis¶

# We'll apply PCA to reduce the dimensionality of our feature space and analyze its benefits.
# One-hot encode categorical features for PCA
X_encoded = pd.get_dummies(X, columns=categorical_features)

# Standardize numerical features
scaler = StandardScaler()
X_numerical = X[numerical_features]
X_numerical_scaled = scaler.fit_transform(X_numerical)

# Replace original numerical columns with scaled versions
for i, col in enumerate(numerical_features):
    X_encoded[col] = X_numerical_scaled[:, i]

# Apply PCA
pca = PCA(random_state=42)
X_pca = pca.fit_transform(X_encoded)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance by PCA Components")
plt.grid(True)
plt.savefig('pca_explained_variance.png')
plt.show()

# Determine number of components for 95% variance
n_components_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1
print(f"\nNumber of components needed for 95% variance: {n_components_95}")
print(f"Original number of features after encoding: {X_encoded.shape[1]}")
print(f"Dimensionality reduction: {X_encoded.shape[1]} -> {n_components_95} features")


# Apply PCA with the determined number of components
pca_95 = PCA(n_components=n_components_95, random_state=42)
X_pca_95 = pca_95.fit_transform(X_encoded)

# Visualize the first two principal components
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_binary, 
                     cmap='viridis', alpha=0.5, edgecolors='w', linewidth=0.5)
plt.colorbar(scatter, label='Severity (Binary)')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA: First Two Principal Components by Accident Severity")
plt.savefig('pca_visualization.png')
plt.show()


### 9. Overfitting/Underfitting Analysis

#We'll analyze the bias-variance tradeoff in our models using learning curves.


from sklearn.model_selection import learning_curve

# Take a smaller subset for learning curves to make computation faster
sample_size = 10000
sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
X_train_sample = X_train.iloc[sample_indices]
y_train_sample = y_train_binary.iloc[sample_indices]

# Create pipelines for both models
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

svm_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42))
])

# Function to plot learning curves
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure(figsize=(10, 6))
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    
    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    plt.legend(loc="best")
    return plt

# Plot learning curves for Random Forest
plot_learning_curve(
    rf_pipeline, "Learning Curve - Random Forest", 
    X_train_sample, y_train_sample, ylim=(0.7, 1.01), cv=3, n_jobs=-1
)
plt.savefig('rf_learning_curve.png')
plt.show()

# Plot learning curves for SVM
plot_learning_curve(
    svm_pipeline, "Learning Curve - SVM", 
    X_train_sample, y_train_sample, ylim=(0.7, 1.01), cv=3, n_jobs=-1
)
plt.savefig('svm_learning_curve.png')
plt.show()


'''
10. Conclusion
Key Findings

    Most Important Features for Predicting Accident Severity:
        Speed limit
        Number of casualties
        Urban/rural area
        Road type
        Light conditions

    Best Classification Approach:
        Random Forest consistently outperformed other models across different metrics
        SVM performed well but showed more tendency toward overfitting
        Decision Tree, Logistic Regression, and KNN had lower performance

    SVM for Binary Classification:
        SVM effectively classified accidents into severe (churn) and non-severe (loyal) categories
        Class weighting helped address the class imbalance issue
        Random Forest still slightly outperformed SVM in binary classification

    Dimensionality Reduction Benefits:
        PCA reduced dimensions from 56 to 10 while preserving 95% of variance
        Significant computational efficiency gains, especially for SVM
        Helped address multicollinearity and potential overfitting
        Enabled effective visualization of high-dimensional data

Recommendations

    Model Selection: Use Random Forest for accident severity prediction due to its superior performance and better handling of class imbalance.

    Feature Engineering: Focus on the top features identified in our analysis, particularly speed limit, number of casualties, and area type.

    Class Imbalance: Apply techniques like SMOTE or class weighting to improve prediction of minority classes (Fatal and Serious accidents).

    Dimensionality Reduction: Apply PCA before model training, especially for computationally intensive models like SVM.

    Hyperparameter Tuning: Regularly optimize model hyperparameters to balance bias and variance.
'''






**********************************************************************

import pandas as pd
import numpy as np
import statistics as stats
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import OrdinalEncoder

import warnings
warnings.filterwarnings('ignore') # We can suppress the warnings

# Load the dataset
df = pd.read_csv("/Users/eupirate/Desktop/CCT/CCT_mac_College_DA_L8/Lectures/Term1/ML/ML_CA_1/archive/Accident_Information_2010.csv")

df.head()


## df.info()
## df.describe()
## df.isnull().sum()
## df.isnull().sum().sum()

## Check and visualize missing values
missing_values = df.drop(columns=["Carriageway_Hazards", "Special_Conditions_at_Site","2nd_Road_Class"], errors="ignore").isnull().sum()
missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

## Plot missing values
plt.figure(figsize=(12, 6))
sns.barplot(x=missing_values.values, y=missing_values.index, palette="magma")
plt.title("Missing Values per Feature")
plt.xlabel("Number of Missing Values")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

## Recalculate missing values
## Excluding features "Carriageway_Hazards", "Special_Conditions_at_Site" and "2nd_Road_Class". "Unclassified" and "NONE" are valid values

excluded_cols = ["Carriageway_Hazards", "Special_Conditions_at_Site","2nd_Road_Class"]
df_excluded = df.drop(columns=excluded_cols, errors="ignore")

# Compute missing percent per column
missing_percent = df_excluded.isnull().mean() * 100
missing_percent_sorted = missing_percent[missing_percent > 0].sort_values(ascending=False)

# Compute overall missing rate after exclusion
total_cells = np.product(df_excluded.shape)
total_missing = df_excluded.isnull().sum().sum()
overall_missing_rate = round((total_missing / total_cells) * 100, 2)

# Create summary DataFrame
missing_summary = pd.DataFrame({
    "Missing %": missing_percent_sorted.round(2)
})

missing_summary, overall_missing_rate


## Treat 2nd_Road_Number as numeric and fill NA with 0

df["2nd_Road_Number"].fillna(0, inplace=True)

## Double check that no NA remains and display value counts (top 5)
## Convert 0 to 0.0 - a better representation it's not missing value
na_check = df["2nd_Road_Number"].isna().sum()
value_counts = df["2nd_Road_Number"].value_counts().head()

na_check, value_counts


## Replace "LSOA_of_Accident_Location" blank space with NaN, and apply with mode()
df["LSOA_of_Accident_Location"].replace("", np.nan, inplace=True)
most_common_lsoa = df["LSOA_of_Accident_Location"].mode()[0]
df["LSOA_of_Accident_Location"].fillna(most_common_lsoa, inplace=True)

### df.isnull().sum()
### "2nd_Road_Class" "Carriageway_Hazards" and "Special_Conditions_at_Site" 's missing values are all valid based on context of this dataset, not missing values

#df.info()


## Check for numeric columns
numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns

## Boxplot to identify potential outliers for numeric columns
plt.figure(figsize=(14, 8))
df[numeric_cols].boxplot(rot=90)
plt.title("Boxplot for Numeric Features to Check Outliers")
plt.tight_layout()
plt.show()

## Examine target variable distribution
severity_counts = df["Accident_Severity"].value_counts()
print("Accident Severity Distribution:")
print(severity_counts)
print("\nPercentage:")
print(severity_counts / len(df) * 100)

## Visualize target variable distribution
plt.figure(figsize=(10, 6))
sns.countplot(x="Accident_Severity", data=df, palette="viridis")
plt.title("Distribution of Accident Severity")
plt.xlabel("Severity")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig("severity_distribution.png")
plt.show()


## Feature Analysis

# Convert date and time to usable features
df["Date"] = pd.to_datetime(df["Date"], format="%d/%m/%Y", errors="coerce")
df["Month"] = df["Date"].dt.month
df["Time"] = pd.to_datetime(df["Time"], format="%H:%M", errors="coerce")
df["Hour"] = df["Time"].dt.hour

# Temporal patterns
plt.figure(figsize=(12, 6))
monthly_accidents = df.groupby(["Month", "Accident_Severity"]).size().unstack()
monthly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Month and Severity")
plt.xlabel("Month")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("accidents_by_month.png")
plt.show()

plt.figure(figsize=(14, 6))
hourly_accidents = df.groupby(["Hour", "Accident_Severity"]).size().unstack()
hourly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Hour and Severity")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('accidents_by_hour.png')
plt.show()


## Relationship between severity and road conditions
plt.figure(figsize=(12, 6))
road_cond = pd.crosstab(df["Road_Surface_Conditions"], df["Accident_Severity"], normalize="index") * 100
road_cond.plot(kind="bar")
plt.title("Accident Severity by Road Surface Conditions")
plt.xlabel("Road Surface Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
#plt.savefig("severity_by_road_conditions.png")
plt.show()


## Relationship between severity and light conditions
plt.figure(figsize=(12, 6))
light_cond = pd.crosstab(df["Light_Conditions"], df["Accident_Severity"], normalize="index") * 100
light_cond.plot(kind="bar")
plt.title("Accident Severity by Light Conditions")
plt.xlabel("Light Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
#plt.savefig("severity_by_light_conditions.png")
plt.show()


## Relationship between severity and speed limit
plt.figure(figsize=(10, 6))
speed_sev = pd.crosstab(df["Speed_limit"], df["Accident_Severity"], normalize="index") * 100
speed_sev.plot(kind="bar")
plt.title("Accident Severity by Speed Limit")
plt.xlabel("Speed Limit (mph)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
#plt.savefig("severity_by_speed_limit.png")
plt.show()

## Relationship between severity and urban/rural area
plt.figure(figsize=(10, 6))
area_sev = pd.crosstab(df["Urban_or_Rural_Area"], df["Accident_Severity"], normalize="index") * 100
area_sev.plot(kind="bar")
plt.title("Accident Severity by Area Type")
plt.xlabel("Area Type (1: Urban, 2: Rural)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
#plt.savefig("severity_by_area.png")
plt.show()

***************************************************************

## Simplified version



# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import PCA

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 8)
plt.rcParams["font.size"] = 12


# Load the dataset
df = pd.read_csv("/Users/eupirate/Desktop/CCT/CCT_mac_College_DA_L8/Lectures/Term1/ML/ML_CA_1/archive/Accident_Information_2010.csv")

# Display basic information
print(f"Dataset shape: {df.shape}")
print(f"\nFirst 5 rows:")
df.head()


# Check column types and missing values
print("Data types:")
print(df.dtypes)

print("\nMissing values:")
print(df.isnull().sum())


# Examine target variable distribution
severity_counts = df["Accident_Severity"].value_counts()
print("Accident Severity Distribution:")
print(severity_counts)
print("\nPercentage:")
print(severity_counts / len(df) * 100)

# Visualize target variable distribution
plt.figure(figsize=(10, 6))
sns.countplot(x="Accident_Severity", data=df, palette="viridis")
plt.title("Distribution of Accident Severity")
plt.xlabel("Severity")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('severity_distribution.png')
plt.show()

# Convert date and time to usable features
df["Date"] = pd.to_datetime(df["Date"], format='%d/%m/%Y', errors='coerce')
df["Month"] = df["Date"].dt.month
df["Time"] = pd.to_datetime(df["Time"], format='%H:%M', errors='coerce')
df["Hour"] = df["Time"].dt.hour

# Temporal patterns
plt.figure(figsize=(12, 6))
monthly_accidents = df.groupby(["Month", "Accident_Severity"]).size().unstack()
monthly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Month and Severity")
plt.xlabel("Month")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('accidents_by_month.png')
plt.show()

plt.figure(figsize=(14, 6))
hourly_accidents = df.groupby(["Hour", "Accident_Severity"]).size().unstack()
hourly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Hour and Severity")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('accidents_by_hour.png')
plt.show()


# Relationship between severity and road conditions
plt.figure(figsize=(12, 6))
road_cond = pd.crosstab(df["Road_Surface_Conditions"], df["Accident_Severity"], normalize="index") * 100
road_cond.plot(kind="bar")
plt.title("Accident Severity by Road Surface Conditions")
plt.xlabel("Road Surface Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('severity_by_road_conditions.png')
plt.show()

# Relationship between severity and light conditions
plt.figure(figsize=(12, 6))
light_cond = pd.crosstab(df["Light_Conditions"], df["Accident_Severity"], normalize="index") * 100
light_cond.plot(kind="bar")
plt.title("Accident Severity by Light Conditions")
plt.xlabel("Light Condition")
plt.ylabel("Percentage")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('severity_by_light_conditions.png')
plt.show()



# Relationship between severity and speed limit
plt.figure(figsize=(10, 6))
speed_sev = pd.crosstab(df["Speed_limit"], df["Accident_Severity"], normalize="index") * 100
speed_sev.plot(kind="bar")
plt.title("Accident Severity by Speed Limit")
plt.xlabel("Speed Limit (mph)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('severity_by_speed_limit.png')
plt.show()

# Relationship between severity and urban/rural area
plt.figure(figsize=(10, 6))
area_sev = pd.crosstab(df["Urban_or_Rural_Area"], df["Accident_Severity"], normalize="index") * 100
area_sev.plot(kind="bar")
plt.title("Accident Severity by Area Type")
plt.xlabel("Area Type (1: Urban, 2: Rural)")
plt.ylabel("Percentage")
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('severity_by_area.png')
plt.show()


# Define target variable
severity_mapping = {'Fatal': 2, 'Serious': 1, 'Slight': 0}
df["Severity_Encoded"] = df["Accident_Severity"].map(severity_mapping)

# Create binary target variable (Severe vs. Non-severe)
# In the context of the assignment:
# - Loyal customers = Non-severe accidents (Slight = 0)
# - Churn customers = Severe accidents (Serious/Fatal = 1)
df["Severity_Binary"] = (df["Severity_Encoded"] > 0).astype(int)

# Select features based on EDA
selected_features = [
    "Day_of_Week", "Hour", "Month", 
    "Road_Type", "Road_Surface_Conditions", "Light_Conditions",
    "Weather_Conditions", "Urban_or_Rural_Area", 
    "Speed_limit", "Number_of_Vehicles", "Number_of_Casualties"
]

# Create feature matrix X and target vectors y
X = df[selected_features].copy()
y = df["Severity_Encoded"]  # For multiclass
y_binary = df["Severity_Binary"]  # For binary classification

# Identify categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical features:", categorical_features)
print("Numerical features:", numerical_features)


# Create preprocessing pipeline
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ('cat', categorical_transformer, categorical_features),
    ('num', numerical_transformer, numerical_features)
])

# Create multiple training splits
split_sizes = [0.05, 0.10, 0.20]
data_splits = {}

for split_size in split_sizes:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_binary, test_size=1-split_size, random_state=42, stratify=y_binary
    )
    
    # Apply preprocessing
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)
    
    data_splits[split_size] = {
        'X_train': X_train_processed,
        'X_test': X_test_processed,
        'y_train': y_train,
        'y_test': y_test
    }
    
    print(f"Split {split_size*100}%: Training samples = {X_train.shape[0]}, Test samples = {X_test.shape[0]}")


# Use the 20% split for feature selection
X_train_20pct = data_splits[0.20]['X_train']
y_train_20pct = data_splits[0.20]['y_train']

# Univariate feature selection
k_best = 8  # Select top 8 features
selector_f = SelectKBest(f_classif, k=k_best)
X_kbest = selector_f.fit_transform(X_train_20pct, y_train_20pct)

# Get selected feature indices
selected_indices = selector_f.get_support(indices=True)
print(f"Selected feature indices: {selected_indices}")
print(f"Number of selected features: {len(selected_indices)}")

# Apply feature selection to all splits
for split_size in split_sizes:
    data_splits[split_size]['X_train_selected'] = data_splits[split_size]['X_train'][:, selected_indices]
    data_splits[split_size]['X_test_selected'] = data_splits[split_size]['X_test'][:, selected_indices]




# Define models to evaluate
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42)
}

# Function to evaluate models
def evaluate_model(model, X_train, y_train, X_test, y_test, cv=5):
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # Perform cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    
    return {
        'accuracy': accuracy,
        'cv_mean': np.mean(cv_scores),
        'cv_std': np.std(cv_scores),
        'report': report,
        'predictions': y_pred
    }

# Results storage
results = {}

# Evaluate models on different splits
for split_size, data in data_splits.items():
    # Extract data
    X_train = data['X_train_selected']
    X_test = data['X_test_selected']
    y_train = data['y_train']
    y_test = data['y_test']
    
    print(f"\nEvaluating models on {split_size*100}% training split")
    print(f"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")
    
    # Evaluate each model
    split_results = {}
    for name, model in models.items():
        print(f"Training {name}...")
        split_results[name] = evaluate_model(model, X_train, y_train, X_test, y_test)
        print(f"{name} - Test Accuracy: {split_results[name]['accuracy']:.4f}, CV Accuracy: {split_results[name]['cv_mean']:.4f} ± {split_results[name]['cv_std']:.4f}")
    
    results[split_size] = split_results



# Create summary DataFrame
summary_data = []
for split_size, split_results in results.items():
    for model_name, model_results in split_results.items():
        summary_data.append({
            'Split Size': f'{split_size*100}%',
            'Model': model_name,
            'Test Accuracy': model_results['accuracy'],
            'CV Mean Accuracy': model_results['cv_mean'],
            'CV Std': model_results['cv_std'],
            'Precision': model_results['report']['1']['precision'],
            'Recall': model_results['report']['1']['recall'],
            'F1 Score': model_results['report']['1']['f1-score']
        })

summary_df = pd.DataFrame(summary_data)
summary_df



# Model comparison across splits
plt.figure(figsize=(14, 8))
for model_name in models.keys():
    model_data = summary_df[summary_df['Model'] == model_name]
    plt.plot(model_data['Split Size'], model_data['Test Accuracy'], marker='o', label=model_name)

plt.xlabel('Training Split Size')
plt.ylabel('Test Accuracy')
plt.title('Model Performance Across Different Training Split Sizes')
plt.legend()
plt.grid(True)
plt.savefig('model_comparison_splits.png')
plt.show()



# Model comparison for 20% split
plt.figure(figsize=(12, 8))
split_20_data = summary_df[summary_df['Split Size'] == '20.0%']
metrics = ['Test Accuracy', 'Precision', 'Recall', 'F1 Score']
x = np.arange(len(models))
width = 0.2

for i, metric in enumerate(metrics):
    plt.bar(x + i*width, split_20_data[metric], width, label=metric)

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model Performance Metrics (20% Training Split)')
plt.xticks(x + width*1.5, split_20_data['Model'])
plt.legend()
plt.grid(axis='y')
plt.savefig('model_metrics_comparison.png')
plt.show()


# Use the 20% split for hyperparameter optimization
X_train_selected = data_splits[0.20]['X_train_selected']
X_test_selected = data_splits[0.20]['X_test_selected']
y_train = data_splits[0.20]['y_train']
y_test = data_splits[0.20]['y_test']

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 1. Logistic Regression Hyperparameter Tuning
print('\n1. Logistic Regression Hyperparameter Tuning')
lr_param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'class_weight': [None, 'balanced']
}

lr_grid = GridSearchCV(
    LogisticRegression(random_state=42, max_iter=1000),
    param_grid=lr_param_grid,
    cv=cv,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

lr_grid.fit(X_train_selected, y_train)
print(f'Best parameters: {lr_grid.best_params_}')
print(f'Best cross-validation score: {lr_grid.best_score_:.4f}')

# Evaluate best model
best_lr = lr_grid.best_estimator_
y_pred_lr = best_lr.predict(X_test_selected)
lr_accuracy = accuracy_score(y_test, y_pred_lr)
print(f'Test accuracy with best parameters: {lr_accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred_lr))


# 2. SVM Hyperparameter Tuning
print('\n2. SVM Hyperparameter Tuning')
svm_param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1],
    'kernel': ['rbf', 'linear', 'poly'],
    'class_weight': ['balanced', None]
}

svm_grid = GridSearchCV(
    SVC(probability=True, random_state=42),
    param_grid=svm_param_grid,
    cv=cv,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

svm_grid.fit(X_train_selected, y_train)
print(f'Best parameters: {svm_grid.best_params_}')
print(f'Best cross-validation score: {svm_grid.best_score_:.4f}')

# Evaluate best model
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test_selected)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
print(f'Test accuracy with best parameters: {svm_accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred_svm))



# Compare optimized models
print('\nComparison of Optimized Models:')
print(f'Logistic Regression: {lr_accuracy:.4f}')
print(f'SVM: {svm_accuracy:.4f}')

# Create confusion matrices
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
cm_lr = confusion_matrix(y_test, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Non-severe', 'Severe'],
            yticklabels=['Non-severe', 'Severe'])
plt.title('Logistic Regression Confusion Matrix')

plt.subplot(1, 2, 2)
cm_svm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Non-severe', 'Severe'],
            yticklabels=['Non-severe', 'Severe'])
plt.title('SVM Confusion Matrix')

plt.tight_layout()
plt.savefig('optimized_models_confusion_matrices.png')
plt.show()



# Create ROC curves for both models
y_prob_lr = best_lr.predict_proba(X_test_selected)[:, 1]
y_prob_svm = best_svm.predict_proba(X_test_selected)[:, 1]

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')
plt.plot(fpr_svm, tpr_svm, color='red', lw=2, label=f'SVM (AUC = {roc_auc_svm:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison: Logistic Regression vs SVM')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.7)
plt.savefig('binary_classification_roc_comparison.png')
plt.show()

# Apply PCA to the 20% training split
X_train_full = data_splits[0.20]['X_train']

# Apply PCA
pca = PCA(random_state=42)
X_pca = pca.fit_transform(X_train_full)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance by PCA Components")
plt.grid(True)
plt.savefig('pca_explained_variance.png')
plt.show()

# Determine number of components for 95% variance
n_components_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1
print(f"\nNumber of components needed for 95% variance: {n_components_95}")
print(f"Original number of features after preprocessing: {X_train_full.shape[1]}")
print(f"Dimensionality reduction: {X_train_full.shape[1]} -> {n_components_95} features")

# Apply PCA with the determined number of components
pca_95 = PCA(n_components=n_components_95, random_state=42)
X_train_pca = pca_95.fit_transform(X_train_full)
X_test_pca = pca_95.transform(data_splits[0.20]['X_test'])

# Train SVM on PCA-transformed data
svm_pca = SVC(probability=True, random_state=42, **svm_grid.best_params_)
svm_pca.fit(X_train_pca, y_train)

# Evaluate SVM on PCA-transformed data
y_pred_pca = svm_pca.predict(X_test_pca)
pca_accuracy = accuracy_score(y_test, y_pred_pca)
print(f'SVM with PCA - Test Accuracy: {pca_accuracy:.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred_pca))



# Compare SVM with and without PCA
print('\nComparison of SVM with and without PCA:')
print(f'SVM without PCA: {svm_accuracy:.4f}')
print(f'SVM with PCA: {pca_accuracy:.4f}')

# Create ROC curve for SVM with PCA
y_prob_pca = svm_pca.predict_proba(X_test_pca)[:, 1]
fpr_pca, tpr_pca, _ = roc_curve(y_test, y_prob_pca)
roc_auc_pca = auc(fpr_pca, tpr_pca)

plt.figure(figsize=(8, 6))
plt.plot(fpr_svm, tpr_svm, color='red', lw=2, label=f'SVM without PCA (AUC = {roc_auc_svm:.2f})')
plt.plot(fpr_pca, tpr_pca, color='green', lw=2, label=f'SVM with PCA (AUC = {roc_auc_pca:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: SVM with vs. without PCA')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.7)
plt.savefig('svm_pca_roc_comparison.png')
plt.show()


from sklearn.model_selection import learning_curve

# Take a smaller subset for learning curves to make computation faster
sample_size = 10000
sample_indices = np.random.choice(X_train_selected.shape[0], min(sample_size, X_train_selected.shape[0]), replace=False)
X_train_sample = X_train_selected[sample_indices]
y_train_sample = y_train.iloc[sample_indices]

# Function to plot learning curves
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure(figsize=(10, 6))
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    
    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    plt.legend(loc="best")
    return plt

# Plot learning curves for Logistic Regression
plot_learning_curve(
    best_lr, "Learning Curve - Logistic Regression", 
    X_train_sample, y_train_sample, ylim=(0.7, 1.01), cv=3, n_jobs=-1
)
plt.savefig('lr_learning_curve.png')
plt.show()

# Plot learning curves for SVM
plot_learning_curve(
    best_svm, "Learning Curve - SVM", 
    X_train_sample, y_train_sample, ylim=(0.7, 1.01), cv=3, n_jobs=-1
)
plt.savefig('svm_learning_curve.png')
plt.show()



************************************
## 15/04/2025
## local versions

import pandas as pd
import numpy as np
import statistics as stats
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import PCA

import warnings
warnings.filterwarnings("ignore")

# Load the dataset
df = pd.read_csv("/Users/eupirate/Desktop/CCT/CCT_mac_College_DA_L8/Lectures/Term1/ML/ML_CA_1/archive/Accident_Information_2010.csv")

df.head()

##df.info()
## df.describe()
## df.isnull().sum()
df.isnull().sum().sum()

## Check and visualize missing values
## Visualize missing values excluding valid-value columns
excluded_cols = ["Carriageway_Hazards", "Special_Conditions_at_Site", "2nd_Road_Class"]
df_excluded = df.drop(columns=excluded_cols, errors="ignore")
missing_values = df_excluded.isnull().sum()
missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x=missing_values.values, y=missing_values.index, palette="magma")
plt.title("Missing Values per Feature")
plt.xlabel("Number of Missing Values")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

## Summary of missing value percentage
missing_percent = df_excluded.isnull().mean() * 100
missing_percent_sorted = missing_percent[missing_percent > 0].sort_values(ascending=False)
total_cells = np.product(df_excluded.shape)
total_missing = df_excluded.isnull().sum().sum()
overall_missing_rate = round((total_missing / total_cells) * 100, 2)
missing_summary = pd.DataFrame({"Missing %": missing_percent_sorted.round(2)})
print(missing_summary)
print("Overall Missing Rate:", overall_missing_rate, "%")


## Handle missing values for key features
## 2nd_Road_Number: fill with 0
if "2nd_Road_Number" in df.columns:
    df["2nd_Road_Number"].fillna(0, inplace=True)


## Convert blank in LSOA to NaN, then impute with mode
if "LSOA_of_Accident_Location" in df.columns:
    df["LSOA_of_Accident_Location"].replace("", np.nan, inplace=True)
    most_common_lsoa = df["LSOA_of_Accident_Location"].mode()[0]
    df["LSOA_of_Accident_Location"].fillna(most_common_lsoa, inplace=True)


### df.isnull().sum()
### "2nd_Road_Class" "Carriageway_Hazards" and "Special_Conditions_at_Site" 's missing values are all valid based on context of this dataset, not missing values


## Boxplot for outlier detection
numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns
plt.figure(figsize=(14, 8))
df[numeric_cols].boxplot(rot=90)
plt.title("Boxplot for Numeric Features to Check Outliers")
plt.tight_layout()
plt.show()


## Distribution of Accident Severity
severity_counts = df["Accident_Severity"].value_counts()
print("Accident Severity Distribution:")
print(severity_counts)
print("\nPercentage:")
print(severity_counts / len(df) * 100)

plt.figure(figsize=(10, 6))
sns.countplot(x="Accident_Severity", data=df, palette="viridis")
plt.title("Distribution of Accident Severity")
plt.xlabel("Severity")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig("severity_distribution.png")
plt.show()


## Extract Month and Hour
df["Date"] = pd.to_datetime(df["Date"], format="%d/%m/%Y", errors="coerce")
df["Month"] = df["Date"].dt.month
df["Time"] = pd.to_datetime(df["Time"], format="%H:%M", errors="coerce")
df["Hour"] = df["Time"].dt.hour


## Monthly trend
plt.figure(figsize=(12, 6))
monthly_accidents = df.groupby(["Month", "Accident_Severity"]).size().unstack()
monthly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Month and Severity")
plt.xlabel("Month")
plt.ylabel("Number of Accidents")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("accidents_by_month.png")
plt.show()


## Hourly trend
plt.figure(figsize=(14, 6))
hourly_accidents = df.groupby(["Hour", "Accident_Severity"]).size().unstack()
hourly_accidents.plot(kind="bar", stacked=True)
plt.title("Accidents by Hour and Severity")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Accidents")
plt.tight_layout()
plt.savefig("accidents_by_hour.png")
plt.show()


## Road condition relationship
plt.figure(figsize=(12, 6))
road_cond = pd.crosstab(df["Road_Surface_Conditions"], df["Accident_Severity"], normalize="index") * 100
road_cond.plot(kind="bar")
plt.title("Accident Severity by Road Surface Conditions")
plt.xlabel("Road Surface Condition")
plt.ylabel("Percentage")
plt.tight_layout()
plt.show()


## Light conditions
plt.figure(figsize=(12, 6))
light_cond = pd.crosstab(df["Light_Conditions"], df["Accident_Severity"], normalize="index") * 100
light_cond.plot(kind="bar")
plt.title("Accident Severity by Light Conditions")
plt.xlabel("Light Condition")
plt.ylabel("Percentage")
plt.tight_layout()
plt.show()


## Speed limit
plt.figure(figsize=(10, 6))
speed_sev = pd.crosstab(df["Speed_limit"], df["Accident_Severity"], normalize="index") * 100
speed_sev.plot(kind="bar")
plt.title("Accident Severity by Speed Limit")
plt.xlabel("Speed Limit (mph)")
plt.ylabel("Percentage")
plt.tight_layout()
plt.show()


## Urban or Rural
plt.figure(figsize=(10, 6))
area_sev = pd.crosstab(df["Urban_or_Rural_Area"], df["Accident_Severity"], normalize="index") * 100
area_sev.plot(kind="bar")
plt.title("Accident Severity by Area Type")
plt.xlabel("Area Type (1: Urban, 2: Rural)")
plt.ylabel("Percentage")
plt.tight_layout()
plt.show()


## Define predictors and target
selected_features = [
    "Day_of_Week", "Hour", "Month", 
    "Road_Type", "Road_Surface_Conditions", "Light_Conditions",
    "Weather_Conditions", "Urban_or_Rural_Area", 
    "Speed_limit", "Number_of_Vehicles", "Number_of_Casualties"
]

## Create binary target: 0 = Slight (loyal), 1 = Serious or Fatal (churn)
df["Severity_Binary"] = df["Accident_Severity"].map({"Slight": 0, "Serious": 1, "Fatal": 1})

## Confirm it worked
print("Binary Target Distribution (0: Slight, 1: Serious/Fatal):")
print(df["Severity_Binary"].value_counts())
print("\nPercentage Breakdown:")
print(df["Severity_Binary"].value_counts(normalize=True) * 100)


X = df[selected_features].copy()
y = df["Severity_Binary"]


## Data 80/20 Split - standard practice
## Ensures proportion of "Slight", "Serious", "Fatal" is preserved in both sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
print("Shapes:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)
print("y_train:", y_train.shape, "y_test:", y_test.shape)

## Preprocessing Pipeline
## Numerical: median imputation + scaling
## Categorical: most_frequent imputation + one-hot encoding

categorical_features = X.select_dtypes(include=["object"]).columns.tolist()
numerical_features = X.select_dtypes(include=["int64", "float64"]).columns.tolist()

numerical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", numerical_pipeline, numerical_features),
    ("cat", categorical_pipeline, categorical_features)
])


## Next sensible step: Feature Selection to find the most predictive features X for the target feature Accident_Severity

## from sklearn.tree import DecisionTreeClassifier
## Use tree based decision tree model, this appraoch handles both numerical and categorical features,
## This step helps identify which features most influence accident severity.
## The visualization will help to identify Top 10 Important Features Predicting Accident Severity

# 🕒 Start timer
start_time = time.time()

importances_df = pd.DataFrame()

## Transform full X for feature importance
X_processed = preprocessor.fit_transform(X)
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_processed, y)

## Get feature names after preprocessing
onehot_cols = preprocessor.named_transformers_["cat"]["encoder"].get_feature_names_out(categorical_features)
all_feature_names = np.concatenate([numerical_features, onehot_cols])

## Get importance scores
feature_importances = pd.Series(dt_model.feature_importances_, index=all_feature_names)
top_features = feature_importances.sort_values(ascending=False).head(10)


# 🕒 End timer
end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Feature Importance Score block took {elapsed_time:.2f} seconds to run.")


## Plot top 10 features
plt.figure(figsize=(10, 6))
top_features.plot(kind="barh", color="teal")
plt.title("Top 10 Important Features Predicting Accident Severity")
plt.xlabel("Feature Importance Score")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()


from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from scipy.stats import ttest_rel
import numpy as np

# 🕒 Start timer
start_time = time.time()

## Refit the preprocessor on X_train
X_processed = preprocessor.fit_transform(X_train)
y_array = y_train.to_numpy()

## Ensure alignment between features and labels
## "Mismatch between X and y"
assert X_processed.shape[0] == len(y_array)

## Reduce dataset size to speed up training and cross-validation
## Justification:
## The full dataset is large (~115k rows), causing long runtimes
## For model comparison (not final training), a smaller random sample (e.g., 5,000) provides a statistically reliable estimate

sample_size = min(5000, X_processed.shape[0])
sample_indices = np.random.choice(X_processed.shape[0], sample_size, replace=False)

## subset X and y
X_sample = X_processed[sample_indices]
y_sample = y_array[sample_indices]

## Define models for comparison
## Logistic Regression 
lr = LogisticRegression(max_iter=5000, random_state=42)
## Support Vector Machine
svm = SVC(kernel="linear", probability=False, max_iter=5000, random_state=42)

## Cross-validation on the smaller sample
cv_scores_lr = cross_val_score(lr, X_sample, y_sample, cv=3, scoring="accuracy")
cv_scores_svm = cross_val_score(svm, X_sample, y_sample, cv=3, scoring="accuracy")

## Output results
print("Logistic Regression CV Accuracy:", np.round(cv_scores_lr, 4))
print("SVM (Linear) CV Accuracy:", np.round(cv_scores_svm, 4))

## Paired t-test: Statistical comparison of model performance
stat, p_value = ttest_rel(cv_scores_lr, cv_scores_svm)
print(f"\nPaired t-test: statistic={stat:.4f}, p-value={p_value:.4f}")
if p_value < 0.05:
    print("Statistically significant difference between models")
else:
    print("No statistically significant difference between models")


end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Classification approach block took {elapsed_time:.2f} seconds to run.")


### How to classify the loyal and churn customers using Support Vector Machines rather than Random Forest classifier?
### Have experiemented a number of different sample sizes from 5,000 up to 30,000.
### 30,000 sample sizes seem to give the best trade-off between runtime and performance

# 🕒 Start timer
start_time = time.time()

## Experiment 30,000 for faster runtime rather than the full dataset
sample_size = min(30000, X_train.shape[0])
sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)
X_train_sample = X_train.iloc[sample_indices]
y_train_sample = y_train.iloc[sample_indices]

## Update preprocessing pipeline, we prepare one-hot encoding to convert categorical variables to numerical input 
categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))  # Output must be converted to dense arrays  to be compatible for PCA
])

numerical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

preprocessor = ColumnTransformer([
    ("num", numerical_pipeline, numerical_features),
    ("cat", categorical_pipeline, categorical_features)
])

## Preprocess the sampled training and full test sets using the fitted pipeline
X_train_proc = preprocessor.fit_transform(X_train_sample)
X_test_proc = preprocessor.transform(X_test)


## Before implementing PCA, we have already applied one-hot encoding to convert categorical variables to numerical input 

from sklearn.decomposition import PCA
## Apply PCA to reduce dimensionality, the goal is to reduce noisy features retaining most of the data variance
## Retained 95% of variance
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_proc)
X_test_pca = pca.transform(X_test_proc)
## The new dataset X_train_pca will have fewer columns but still carry most of the important features


## Define and train SVM model (for classification of churn vs loyal) the non-linear kernels
svm_model = SVC(kernel="rbf",             # hyperparameter: kernel type
                probability=True,         # enables probability estimates
                C=1.0, gamma="scale",     # hyperparameter: regularization strength
                class_weight="balanced",  # hyperparameter: kernel coefficient
                random_state=42)          # handles class imbalance

svm_model.fit(X_train_pca, y_train_sample)

## Make predictions
y_pred = svm_model.predict(X_test_pca)

## Evaluate results
print("🔎 Support Vector Machine (SVM) Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Loyal (Slight)", "Churn (Serious/Fatal)"]))

from sklearn.metrics import f1_score, roc_auc_score

## Get predicted probabilities
y_proba = svm_model.predict_proba(X_test_pca)[:, 1]

## Add F1 Score
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.4f}")

# 🕒 End timer
end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Loyal and churn SVM, RF block took {elapsed_time:.2f} seconds to run.")

## Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples",
            xticklabels=["Loyal", "Churn"],
            yticklabels=["Loyal", "Churn"])
plt.title("SVM Confusion Matrix (with PCA)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()



## Random Rorest Model
# 🕒 Start timer
start_time = time.time()

from sklearn.ensemble import RandomForestClassifier

print("\n" + "="*80)
print("RANDOM FOREST IMPLEMENTATION FOR ACCIDENT SEVERITY PREDICTION")
print("="*80)

## Random Forest model hyperparameter setup
rf_model = RandomForestClassifier(
    n_estimators=100,            # number of trees in the forest
    max_depth=None,              # no limit on tree depth; trees grow until all leaves are pure
    min_samples_split=2,         # minimum number of samples required to split an internal node
    min_samples_leaf=1,          # minimum number of samples required at a leaf node
    max_features='sqrt',         # number of features to consider when looking for the best split (√ total)
    bootstrap=True,              # use bootstrap sampling when building trees
    class_weight='balanced',     # automatically adjust weights inversely proportional to class frequencies
    random_state=42,             # ensure reproducibility
    n_jobs=-1                    # use all available CPU cores for faster training
)


##Train model
print("Training Random Forest model...")
rf_model.fit(X_train_pca, y_train_sample)

## Predict on test set
print("Making predictions on test set...")
rf_pred = rf_model.predict(X_test_pca)
rf_proba = rf_model.predict_proba(X_test_pca)[:, 1]

## Evaluation
print("\n Random Forest Classification Report:")
print(classification_report(y_test, rf_pred, target_names=["Loyal (Slight)", "Churn (Serious/Fatal)"]))

rf_accuracy = accuracy_score(y_test, rf_pred)
rf_f1 = f1_score(y_test, rf_pred)
rf_auc = roc_auc_score(y_test, rf_proba)

print(f"Accuracy: {rf_accuracy:.4f}")
print(f"F1 Score: {rf_f1:.4f}")
print(f"ROC-AUC Score: {rf_auc:.4f}")


# 🕒 End timer
end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Random Forest block for accident severity prediction took {elapsed_time:.2f} seconds to run.")


## Confusion Matrix
cm_rf = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Greens",
            xticklabels=["Loyal", "Churn"],
            yticklabels=["Loyal", "Churn"])
plt.title("Random Forest Confusion Matrix (with PCA)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.savefig("rf_confusion_matrix.png")
plt.show()



## Feature Importances (Optional: only if PCA components < 20)
if X_train_pca.shape[1] <= 20:
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title("Random Forest Feature Importances (PCA Components)")
    plt.bar(range(X_train_pca.shape[1]), importances[indices], align="center")
    plt.xticks(range(X_train_pca.shape[1]), [f"PC{i+1}" for i in indices], rotation=90)
    plt.xlim([-1, X_train_pca.shape[1]])
    plt.tight_layout()
    plt.savefig("rf_feature_importance.png")
    plt.show()




## Model Model Performance Comparison 1. "SVM", 2. "Random Forest" 
# 🕒 Start timer
start_time = time.time()

print("\n" + "="*80)
print("MODEL COMPARISON: SVM vs RANDOM FOREST")
print("="*80)

# SVM metrics already computed
svm_accuracy = accuracy_score(y_test, y_pred)
svm_f1 = f1_score(y_test, y_pred)
svm_auc = roc_auc_score(y_test, y_proba)

## Create comparison DataFrame
metrics_df = pd.DataFrame({
    'Model': ['SVM', 'Random Forest'],
    'Accuracy': [svm_accuracy, rf_accuracy],
    'F1 Score': [svm_f1, rf_f1],
    'ROC-AUC': [svm_auc, rf_auc]
})

print(metrics_df)

# 🕒 End timer
end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Model SVM vs Random Forest comparison block took {elapsed_time:.2f} seconds to run.")


## Plotting comparison
metrics_df.set_index("Model")[["Accuracy", "F1 Score", "ROC-AUC"]].plot(kind="bar", figsize=(10, 6))
plt.title("Model Performance Comparison: SVM vs Random Forest")
plt.ylabel("Score")
plt.ylim(0, 1.0)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.savefig("model_comparison.png")
plt.show()


## Model Performance Comparison 1. "Logistic Regression", 2. "SVM", 3. "Random Forest" 

## 🕒 Start timer
start_time = time.time()

## Define scoring metrics using string aliases (instead of make_scorer)
scoring = ["accuracy", "f1", "roc_auc"]

## Sample size 30,000 for cross-validation, not the full dataset (~115k rows)
cv_sample_size = min(30000, X_train.shape[0])
cv_indices = np.random.choice(X_train.shape[0], cv_sample_size, replace=False)
X_cv_sample = X_train.iloc[cv_indices]
y_cv_sample = y_train.iloc[cv_indices]

## Preprocessing pipeline with dense output for PCA compatibility
categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

numerical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

preprocessor = ColumnTransformer([
    ("num", numerical_pipeline, numerical_features),
    ("cat", categorical_pipeline, categorical_features)
])

## Transform CV sample
X_cv_proc = preprocessor.fit_transform(X_cv_sample)

## Dimensionality reduction with PCA
pca = PCA(n_components=0.95, random_state=42)
X_cv_pca = pca.fit_transform(X_cv_proc)


## Define models with baseline hyperparameters
## 1. Logistic Regression
lr_model = LogisticRegression(
    max_iter=5000,        # increase max iterations to ensure convergence ( was 1,000 )
    random_state=42       # seed for reproducibility
)

## 2. Support Vector Machine (RBF kernel)
svm_model = SVC(
    kernel="rbf",               # use Radial Basis Function kernel for non-linear decision boundaries
    probability=True,           # enable probability estimates (required for ROC-AUC score)
    class_weight="balanced",    # adjust weights inversely proportional to class frequencies (handle imbalance)
    random_state=42             # seed for reproducibility
)

## 3. Random Forest Classifier
rf_model = RandomForestClassifier(
    n_estimators=100,           # number of trees in the forest (more trees = more stable prediction)
    class_weight="balanced",    # handle imbalanced classes by assigning higher weight to minority class
    max_features="sqrt",        # number of features to consider at each split (square root of total)
    random_state=42,            # seed for reproducibility
    n_jobs=-1                   # use all available CPU cores for parallel training (faster performance)
)





## Use string-based scoring in cross_validate
## This block performs 5-fold cross-validation on the PCA-transformed data for each model
## using multiple scoring metrics: accuracy, F1 score, and ROC-AUC.

cv_lr = cross_validate(
    lr_model,           # Logistic Regression model
    X_cv_pca,           # Input features (after PCA)
    y_cv_sample,        # Target variable
    cv=5,               # 5-fold cross-validation
    scoring=scoring     # List of scoring metrics to evaluate: accuracy, f1, roc_auc
)

cv_svm = cross_validate(
    svm_model,          # svm model
    X_cv_pca, 
    y_cv_sample, 
    cv=5, 
    scoring=scoring
)

cv_rf = cross_validate(
    rf_model,          # random forest model
    X_cv_pca, 
    y_cv_sample, 
    cv=5, 
    scoring=scoring
)

## Summarize mean scores
## This block creates a DataFrame summarizing the average performance across folds
## for each model and each evaluation metric. It helps in comparing model effectiveness.

cv_summary_df = pd.DataFrame({
    "Model": ["Logistic Regression", "SVM", "Random Forest"],  #  model names
    "Accuracy": [                                               # mean accuracy score for each model
        cv_lr["test_accuracy"].mean(),
        cv_svm["test_accuracy"].mean(),
        cv_rf["test_accuracy"].mean()
    ],
    "F1 Score": [                                               # mean F1 score (harmonic mean of precision and recall)
        cv_lr["test_f1"].mean(),
        cv_svm["test_f1"].mean(),
        cv_rf["test_f1"].mean()
    ],
    "ROC-AUC": [                                                # mean ROC-AUC score (area under the curve)
        cv_lr["test_roc_auc"].mean(),
        cv_svm["test_roc_auc"].mean(),
        cv_rf["test_roc_auc"].mean()
    ]
})


## Print CV results
print("\nCross-Validated Model Performance Summary:")
print(cv_summary_df.round(4))

## 🕒 End timer
end_time = time.time()
elapsed_time = end_time - start_time
print(f"\n⏱️ Cross-validation LR, SVM, RF block took {elapsed_time:.2f} seconds to run.")


## Visualization
plt.figure(figsize=(10, 6))
bar_width = 0.2
x = np.arange(len(cv_summary_df))

plt.bar(x, cv_summary_df["Accuracy"], width=bar_width, label="Accuracy")
plt.bar(x + bar_width, cv_summary_df["F1 Score"], width=bar_width, label="F1 Score")
plt.bar(x + 2 * bar_width, cv_summary_df["ROC-AUC"], width=bar_width, label="ROC-AUC")

plt.xticks(x + bar_width, cv_summary_df["Model"])
plt.ylabel("Score")
plt.ylim(0, 1)
plt.title("Cross-Validated Model Comparison: Logistic Regression vs SVM vs Random Forest")
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)

# 🏷️ Annotate sample size and folds
plt.text(2.1, 0.85, f"{cv_sample_size:,}\ncv=5", ha="center", va="center",
         fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="orange", alpha=0.6))

plt.tight_layout()
plt.savefig("cv_model_comparison.png")
plt.show()





























